---
title: "Лабораторна робота №2. Розвідувальний аналіз даних"
author: "[Зайкін А. В.](https://www.linkedin.com/in/zaykinandrew/), `r format(Sys.time(), '%Y')`"
date: "`r Sys.Date()`"
output:
  html_notebook:
    toc: yes
    toc_float: true
    highlight: tango
    code-fold: true
    code-line-numbers: true
    code-tools: true
    code-copy: true
fontsize: 12pt
lang: ukr
header-includes:
 \usepackage[T2A]{fontenc}
 \usepackage[utf8]{inputenc}
 \usepackage[russian]{babel}
editor_options: 
  chunk_output_type: console
---

# Постановка задачі

### Мета роботи:

1. Засвоєння принципів, знайомство з інструментами та набуття навичок експлораторного (розвідувального) аналізу даних засобами мови програмування R та колекції пакетів `dplyr`, `ggplot2`, `corrplot`, `funModeling`, `rio`, `magrittr`, `knitr`.
2. Побудова моделі зниження розмірності даних засобами мови програмування R та колекції пакетів `FactoMineR`, `factoextra`.

# Короткі теоретичні відомості

### Що таке розвідувальний аналіз даних?
Розвідувальний аналіз даних (Exploratory Data Analysis, EDA) - це процес вивчення даних з метою з'ясування їхніх властивостей, закономірностей та аномалій. EDA є важливою частиною Data Science і використовується для попереднього аналізу даних перед побудовою моделей.

### Що таке зниження розмірності даних?
Зниження розмірності даних (Dimensionality Reduction) - це процес зменшення кількості змінних у наборі даних, зберігаючи при цьому важливу інформацію. Це дозволяє спростити аналіз даних, зменшити витрати на обробку та збереження даних, а також покращити якість моделей машинного навчання.

### Що таке PCA?
Головна компонента (англ. Principal Component, PCA) — це лінійна комбінація вихідних змінних, що має максимальну дисперсію.

# Виконання завдання

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
options(repos = c(CRAN = "https://cran.r-project.org"))

if (!require(devtools)) {
  install.packages("devtools")
  library(devtools)
}

devtools::install_github("husson/FactoMineR")
devtools::install_github("kassambara/factoextra")

packages <- c("rio", "magrittr", "ggplot2", "dplyr", "corrplot", "funModeling", "knitr", "factoextra", "FactoMineR")
lapply(packages, library, character.only = TRUE, quietly = TRUE)
people <- import("data/hw.xlsx")
```

Для початку виведемо перші рядки даних.

```{r}
people %>%
	head()
```

У датафреймі є 3 змінні: `Зріст, см`, `Вага, кг` та `Стать`. Для початку дослідимо статистику даних.

```{r}
people %>%
  df_status()
```

2 змінні мають числову природу, 1 змінна має категоріальну природу. Всі змінні не містять пропущені значення, не містять надвеликих значень. Робимо висновок, що дані комплектні і добре підготовлені до аналізу.

Виведемо гістограми для кожної змінної з урахуванням категоріальної змінної.

```{r}
ggplot(data = people, mapping = aes(x = `Вага, кг`, colour = `Стать`)) +
  geom_freqpoly(binwidth = 1.5)
```

```{r}
ggplot(data = people, mapping = aes(x = `Зріст, см`, colour = `Стать`)) +
  geom_freqpoly(binwidth = 1.5)
```

Загалом гістограми виглядають симетрично розподіленими, за виключенням двох викидів для змінної `Вага, кг`. Вiдфільтруємо дані та побудуємо графік розсіювання.

```{r}
people <- people %>%
  filter(`Вага, кг` < 100)
people %>%
  ggplot(mapping = aes(x = `Зріст, см`, y = `Вага, кг`, colour = `Стать`)) +
  	geom_point()
```

На графіку бачимо, що є дві групи спостережень: чоловіки та жінки. Графіки чоловiкiв та жiнок перетинаються, але зазвичай чоловіки зазвичай вищі та важчі за жінок. Видно, що змiннi коварiюються. Виведемо статистику з групуванням по категоріальній змінній.

```{r}
people %>%
  group_by(`Стать`) %>%
	summarise(
		count = n(),
		mean_height = mean(`Зріст, см`),
		mean_weight = mean(`Вага, кг`),
		median_height = median(`Зріст, см`),
		median_weight = median(`Вага, кг`),
		sd_height = sd(`Зріст, см`),
		sd_weight = sd(`Вага, кг`),
	)
```

Можемо побачити, що середн значення i медiана зросту та ваги по чоловiкам та жiнкам вiдрiзняються приблизно на одне значення. Зрiст чоловiкiв значно менше розподiлений (менша дiсперсiя), нiж зрiст жiнок. Це говорить про те, що перед застосуванням методу факторного аналізу треба провести стандартизацію даних, що поверне нам матрицю кореляцій.

```{r}
# Видiлимо лише числовi змiннi
people_num <- people %>%
	select(-`Стать`)

# Обчислюємо кореляційну матрицю
cor_matrix <- cor(people_num, use = "pairwise.complete.obs")

# Виведемо кореляційну матрицю
cor_matrix %>%
	knitr::kable(caption = "Кореляційна матриця")
```

Можемо побачити, що має місце позитивний кореляційний зв'язок між змінними `Зріст, см` та `Вага, кг`. Тепер застосуємо метод головних компонент (PCA) для зниження розмірності даних.

Спочатку знайдемо головні компоненти.

```{r}
peoplePCA <- PCA(people_num)
eigenvalues <- as.data.frame(peoplePCA$eig)
knitr::kable(eigenvalues, caption = "Головні компоненти")
```

Як можна побачити, що перше власне значення є досить великим. Це означає, що перша компонента пояснює досить велику частину дисперсії. Знайдемо власнi вектори. Цi вектори i будуть нашою матрицею перетворень.

```{r}
eigenvectors <- peoplePCA$var$coord
knitr::kable(round(eigenvectors, 3), caption = "Власні вектори")
```

Побудуємо біплот головних компонент.

```{r warning=FALSE}
fviz_pca_biplot(peoplePCA,
                repel = TRUE,
                warning = FALSE,
                geom = c("point"),
                # label = "none", # hide individual labels
             habillage = as.factor(people$`Стать`), # color by groups
             axes = c(1, 2),
            #  repel = TRUE,
             select.var = list(name = c("Зріст, см", "Вага, кг")),
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE # Concentration ellipses
             )
```

На біплоті ми однозначно бачимо, що дані не є однорідними. Двi групи точок частково перетинаються, але все ж таки займають різні області простору головних компонент. Якщо дивитися на власнi вектори, то можна побачити, що основна диференціація відбувається вздовж першої компоненти. Це означає, що перша компонента поєднує вплив змінних `Зріст, см` та `Вага, кг`. Далi першу компонту ми можемо використовувати для класифікації чоловіків та жінок у регресійних моделях.